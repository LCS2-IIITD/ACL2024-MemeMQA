{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MM BEiT-BERT-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import os, sys\n",
    "sys.path.append('/workout/early-stopping-pytorch')\n",
    "from pytorchtools import EarlyStopping\n",
    "from tqdm import tqdm,trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitModel, AutoFeatureExtractor\n",
    "from transformers import BertTokenizer\n",
    "from transformers import VisualBertForPreTraining\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "BERTtokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeMQACorpus(torch.utils.data.Dataset):\n",
    "    \"\"\"Uses jsonl data to preprocess and serve \n",
    "    dictionary of multimodal tensors for model input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        img_dir,\n",
    "        mode=None,\n",
    "        balance=False,\n",
    "        dev_limit=None,\n",
    "        random_state=0,\n",
    "    ):\n",
    "\n",
    "        self.samples_frame = pd.read_json(\n",
    "            data_path\n",
    "        )\n",
    "        \n",
    "        self.samples_frame = self.samples_frame[self.samples_frame[\"meme_image\"].notnull()]\n",
    "        self.samples_frame = self.samples_frame[self.samples_frame[\"ocr\"].notnull()]\n",
    "        self.samples_frame = self.samples_frame[self.samples_frame[\"entity\"].notnull()]\n",
    "        self.samples_frame = self.samples_frame[self.samples_frame[\"explanation\"].notnull()]\n",
    "        if mode == \"test\":\n",
    "            self.samples_frame = self.samples_frame[self.samples_frame[\"explanation1\"].notnull()]\n",
    "\n",
    "        self.samples_frame = self.samples_frame.reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        self.samples_frame.image = self.samples_frame.apply(\n",
    "            lambda row: (img_dir + '/' + row.meme_image), axis=1\n",
    "        )\n",
    "        self.image_transform = Resize((256,256))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"This method is called when you do len(instance) \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        return len(self.samples_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This method is called when you do instance[key] \n",
    "        for an instance of this class.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = self.samples_frame.loc[idx, \"meme_image\"]  \n",
    "    \n",
    "#         ***Get Beit input data***\n",
    "        file_name = data_dir + self.samples_frame.loc[idx, \"meme_image\"]\n",
    "        beit_image_data = Image.open(file_name)\n",
    "        if beit_image_data.mode != 'RGB':\n",
    "            beit_image_data = beit_image_data.convert('RGB')            \n",
    "        beit_inputs = feature_extractor(images=beit_image_data, return_tensors=\"pt\", padding=True)\n",
    "        beit_inputs['pixel_values'] = beit_inputs['pixel_values'].squeeze()\n",
    "   \n",
    "        bert_inputs = self.samples_frame.loc[idx, \"question\"]  + \"\\n Options: \" + self.samples_frame.loc[idx, \"optC\"] + \"\\nContext: \" + self.samples_frame.loc[idx, \"ocr\"]\n",
    "        \n",
    "        decoder_text = \"Answer: \" + self.samples_frame.loc[idx, \"entity\"] + \" BECAUSE \" + self.samples_frame.loc[idx, \"explanation\"]   \n",
    "            \n",
    "        sample = {\n",
    "                \"img_name\": img_name,\n",
    "                \"bert_inputs\": bert_inputs,\n",
    "                \"beit_inputs\": beit_inputs,\n",
    "                \"decoder_text\": decoder_text\n",
    "            }\n",
    "        try:\n",
    "            sample[\"decoder_text1\"] = \"Answer: \" + self.samples_frame.loc[idx, \"entity\"] + \" BECAUSE \" + self.samples_frame.loc[idx, \"explanation1\"]\n",
    "        except:\n",
    "            pass\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 4\n",
    "train_path = \"ANONYMISED\"\n",
    "dev_path = \"ANONYMISED\"\n",
    "data_dir = \"ANONYMISED\"\n",
    "hm_dataset_train = MemeMQACorpus(train_path, data_dir)\n",
    "dataloader_train = DataLoader(hm_dataset_train, batch_size=BS,\n",
    "                        shuffle=True, num_workers=0)\n",
    "hm_dataset_val = MemeMQACorpus(dev_path, data_dir)\n",
    "dataloader_val = DataLoader(hm_dataset_val, batch_size=BS,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "data_time = AverageMeter('Data', ':6.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pathlib import Path\n",
    "\n",
    "class MM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MM, self).__init__()        \n",
    "        self.model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model_encdec = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\"microsoft/beit-base-patch16-224-pt22k\", \"bert-base-uncased\")\n",
    "        self.model_encdec.config.decoder_start_token_id = BERTtokenizer.cls_token_id\n",
    "        self.model_encdec.config.pad_token_id = BERTtokenizer.pad_token_id\n",
    "        \n",
    "    def forward(self, pixel_values, bert_input_ids, bert_attention_mask, bert_token_type_ids, dec_labels):\n",
    "        bert_encoder_outputs = self.model_bert(input_ids = bert_input_ids, attention_mask = bert_attention_mask, token_type_ids = bert_token_type_ids, output_hidden_states=True, return_dict=True)\n",
    "        enc_dec_out = self.model_encdec(pixel_values=pixel_values, encoder_outputs=bert_encoder_outputs, labels=dec_labels, output_hidden_states=True, return_dict=True)\n",
    "        return enc_dec_out      \n",
    "\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = MM()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "def train_model(model, patience, n_epochs):\n",
    "    epochs = n_epochs\n",
    "    clip = 5\n",
    "    \n",
    "    train_acc_list=[]\n",
    "    val_acc_list=[]\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    train_vencdec_loss_list=[]\n",
    "    val_vencdec_loss_list=[]\n",
    "    train_main_loss_list=[]\n",
    "    val_main_loss_list=[]\n",
    "    \n",
    "    # initialize the experiment path\n",
    "    Path(exp_path).mkdir(parents=True, exist_ok=True)\n",
    "    # initialize early_stopping object\n",
    "    chk_file = os.path.join(exp_path, 'checkpoint_'+exp_name+'.pt')\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=chk_file)\n",
    "\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        print(f\"******************************EPOCH - {i}****************************************\")\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        total_vencdec_loss_train = 0\n",
    "        total_main_loss_train = 0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "\n",
    "        for data in tqdm(dataloader_train, total = len(dataloader_train), desc = \"Mini-batch progress\"):\n",
    "            # print(f'------------------Mini Batch - {mbcnt+1}------------------')\n",
    "            pixel_values_start = time.time()\n",
    "            try:\n",
    "                pixel_values = data['beit_inputs']['pixel_values'].to(device)\n",
    "            except:\n",
    "                print(data)\n",
    "                break\n",
    "            data_time.update(time.time() - pixel_values_start)\n",
    "            if code_prof:\n",
    "                print(f\"pixel_values processing time: {data_time.val}\")\n",
    "            \n",
    "            data_time.reset()\n",
    "            BERT_start = time.time()\n",
    "            BERTtokens = BERTtokenizer(data['bert_inputs'], padding='max_length', truncation=True, max_length=50)\n",
    "\n",
    "            input_ids = torch.tensor(BERTtokens[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.tensor(BERTtokens[\"attention_mask\"]).to(device)\n",
    "            token_type_ids = torch.tensor(BERTtokens[\"token_type_ids\"]).to(device)\n",
    "            data_time.update(time.time() - BERT_start)\n",
    "            if code_prof:\n",
    "                print(f\"BERT processing time: {data_time.val}\")            \n",
    "            \n",
    "            \n",
    "            data_time.reset()\n",
    "            decoder_labels_start = time.time()\n",
    "            decoder_labels = BERTtokenizer(data['decoder_text'], padding=True, return_tensors=\"pt\").to(device).input_ids\n",
    "            data_time.update(time.time() - decoder_labels_start)\n",
    "            if code_prof:\n",
    "                print(f\"decoder_labels processing time: {data_time.val}\")\n",
    "            \n",
    "            label = data['label'].to(device)\n",
    "            model.zero_grad()\n",
    "            data_time.reset()\n",
    "            model_start = time.time()\n",
    "            vencdec_out = model(pixel_values, input_ids, attention_mask, token_type_ids, decoder_labels)\n",
    "            data_time.update(time.time() - model_start)\n",
    "            if code_prof:\n",
    "                print(f\"model processing time: {data_time.val}\")\n",
    "            \n",
    "            vencdec_loss = vencdec_out.loss\n",
    "            loss = vencdec_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_train += label.size(0)\n",
    "                total_vencdec_loss_train += vencdec_loss.item()\n",
    "                total_loss_train += loss.item()\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_loss = 0\n",
    "        train_vencdec_loss = total_vencdec_loss_train/total_train\n",
    "        train_main_loss = 0\n",
    "\n",
    "\n",
    "        # evaluation on validation data phase\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        total_vencdec_loss_val = 0\n",
    "        total_main_loss_val = 0\n",
    "        total_val = 0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader_val:                \n",
    "                pixel_values_start = time.time()\n",
    "                pixel_values = data['beit_inputs']['pixel_values'].to(device)\n",
    "                data_time.update(time.time() - pixel_values_start)\n",
    "                if code_prof:\n",
    "                    print(f\"pixel_values processing time: {data_time.val}\")\n",
    "\n",
    "                data_time.reset()\n",
    "                BERT_start = time.time()\n",
    "                BERTtokens = BERTtokenizer(data['bert_inputs'], padding='max_length', truncation=True, max_length=50)\n",
    "\n",
    "                input_ids = torch.tensor(BERTtokens[\"input_ids\"]).to(device)\n",
    "                attention_mask = torch.tensor(BERTtokens[\"attention_mask\"]).to(device)\n",
    "                token_type_ids = torch.tensor(BERTtokens[\"token_type_ids\"]).to(device)\n",
    "                data_time.update(time.time() - BERT_start)\n",
    "                if code_prof:\n",
    "                    print(f\"BERT processing time: {data_time.val}\")            \n",
    "\n",
    "\n",
    "                data_time.reset()\n",
    "                decoder_labels_start = time.time()\n",
    "                decoder_labels = BERTtokenizer(data['decoder_text'], padding=True, return_tensors=\"pt\").to(device).input_ids\n",
    "                data_time.update(time.time() - decoder_labels_start)\n",
    "                if code_prof:\n",
    "                    print(f\"decoder_labels processing time: {data_time.val}\")\n",
    "                \n",
    "                label_val = data['label'].to(device)\n",
    "                model.zero_grad()\n",
    "                data_time.reset()\n",
    "                model_start = time.time()\n",
    "                vencdec_out_val = model(pixel_values, input_ids, attention_mask, token_type_ids, decoder_labels)\n",
    "                data_time.update(time.time() - model_start)\n",
    "                if code_prof:\n",
    "                    print(f\"model processing time: {data_time.val}\")\n",
    "                \n",
    "                vencdec_loss_val = vencdec_out_val.loss\n",
    "                main_loss_val = 0\n",
    "                loss_val = vencdec_loss_val\n",
    "                total_val += label_val.size(0)\n",
    "                total_vencdec_loss_val += vencdec_loss_val.item()\n",
    "                total_main_loss_val = 0\n",
    "                total_loss_val += loss_val.item()\n",
    "                \n",
    "        print(\"Saving model...\") \n",
    "        torch.save(model.state_dict(), os.path.join(exp_path, \"final.pt\"))\n",
    "\n",
    "        val_acc = 0\n",
    "        val_loss = 0\n",
    "        val_vencdec_loss = total_vencdec_loss_val/total_val\n",
    "        val_main_loss = 0\n",
    "        \n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        train_vencdec_loss_list.append(train_vencdec_loss)\n",
    "        val_vencdec_loss_list.append(total_vencdec_loss_val)\n",
    "        train_main_loss_list.append(train_main_loss)\n",
    "        val_main_loss_list.append(total_main_loss_val)\n",
    "                    \n",
    "        print(f'Epoch {i+1}: train_acc: {train_acc:.4f} | val_acc: {val_acc:.4f} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | train_vencdec_loss: {train_vencdec_loss:.4f} | val_vencdec_loss: {val_vencdec_loss:.4f} | train_main_loss: {train_main_loss:.4f} | val_main_loss: {val_main_loss:.4f}')\n",
    "        with open(os.path.join(exp_path, exp_name+'_base_exp_results.txt'), 'a+') as of:\n",
    "            of.write(f'Epoch {i+1}: train_acc: {train_acc:.4f} | val_acc: {val_acc:.4f} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | train_vencdec_loss: {train_vencdec_loss:.4f} | val_vencdec_loss: {val_vencdec_loss:.4f} | train_main_loss: {train_main_loss:.4f} | val_main_loss: {val_main_loss:.4f}\\n')\n",
    "        \n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return  model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, train_vencdec_loss_list, val_vencdec_loss_list, train_main_loss_list, val_main_loss_list, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prof = False\n",
    "exp_name = \"MM_BEiT_BERT_BERT\"\n",
    "exp_path = \"ANONYMISED\"\n",
    "\n",
    "lr=5e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "n_epochs = 10\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 10\n",
    "\n",
    "model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, train_vencdec_loss_list, val_vencdec_loss_list, train_main_loss_list, val_main_loss_list, i = train_model(model, patience, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    model.eval()\n",
    "    code_prof = False\n",
    "    total_loss_test = 0\n",
    "    total_vencdec_loss_test = 0\n",
    "    total_main_loss_test = 0\n",
    "    total_deBERTa_loss_test = 0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "    generated_result = []\n",
    "    exp1 = []\n",
    "    exp2 = []\n",
    "    ques = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader_test, 0):               \n",
    "            pixel_values_start = time.time()\n",
    "            pixel_values_test = data['beit_inputs']['pixel_values'].to(device)\n",
    "            data_time.update(time.time() - pixel_values_start)\n",
    "            if code_prof:\n",
    "                print(f\"pixel_values processing time: {data_time.val}\")\n",
    "\n",
    "            BERTtokens = BERTtokenizer(data['bert_inputs'], padding='max_length', truncation=True, max_length=50)\n",
    "\n",
    "            input_ids = torch.tensor(BERTtokens[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.tensor(BERTtokens[\"attention_mask\"]).to(device)\n",
    "            token_type_ids = torch.tensor(BERTtokens[\"token_type_ids\"]).to(device)\n",
    "            data_time.reset()\n",
    "            decoder_labels_start = time.time()\n",
    "            data_time.update(time.time() - decoder_labels_start)\n",
    "            if code_prof:\n",
    "                print(f\"decoder_labels processing time: {data_time.val}\")\n",
    "\n",
    "            data_time.reset()\n",
    "            model_start = time.time()            \n",
    "            bert_encoder_outputs_test = model.model_bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            generated_ids = model.model_encdec.generate(pixel_values=pixel_values_test, encoder_outputs=bert_encoder_outputs_test, max_length=150, num_beams = 2, repetition_penalty = 2.5, length_penalty = 1.0)\n",
    "            preds = [BERTtokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            print(preds)\n",
    "            generated_result.extend(preds)\n",
    "            exp1.extend(data[\"decoder_text\"])\n",
    "            exp2.extend(data[\"decoder_text1\"])\n",
    "            ques.extend(data[\"bert_inputs\"])\n",
    "    return generated_result, exp1, exp2, ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"ANONYMISED\"\n",
    "hm_dataset_test = MemeMQACorpus(test_path, data_dir, mode = \"test\")\n",
    "dataloader_test = DataLoader(hm_dataset_test, batch_size=BS,\n",
    "                        shuffle=False, num_workers=0)\n",
    "generated_result, ref1, ref2, ques = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\"hyp\" : generated_result, \"ref1\" : ref1, \"ref2\" : ref2, \"ques\" : ques}\n",
    "df1 = pd.DataFrame(dict)\n",
    "df1.to_csv(exp_name +  \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
